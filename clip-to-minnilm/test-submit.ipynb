{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, lr, warmup_ratio, total_optimization_steps):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "        self.total_optimization_steps = total_optimization_steps\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.backbone = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=1024, out_features=384),\n",
    "        )\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        domain_from, domain_to = batch\n",
    "        domain_to_hat = self(domain_from)\n",
    "        loss = self.criterion(domain_to_hat, domain_to)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        logs = {\"train_loss\": loss.detach().cpu().numpy()}\n",
    "        return {\"loss\": loss, \"log\": logs,}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        domain_from, domain_to = batch\n",
    "        domain_to_hat = self(domain_from)\n",
    "        loss = self.criterion(domain_to_hat, domain_to)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=False)\n",
    "        return {\"val_loss\": loss,}\n",
    "\n",
    "    # def validation_epoch_end(self, outputs):\n",
    "    #     avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "    #     print(\n",
    "    #         \"\\n\\nVAL Loss: {}\\n\".format(\n",
    "    #             avg_loss,\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=0)\n",
    "        lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_ratio * self.total_optimization_steps,\n",
    "            num_training_steps=self.total_optimization_steps,\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = Model.load_from_checkpoint('/home/toomuch/kaggle-diffusion/clip-to-minnilm/runs/-Debug/-Debug-epoch=32-val_loss=0.0011.ckpt')\n",
    "projection = projection.to('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "# import torch \n",
    "openclip_model = AutoModel.from_pretrained('laion/CLIP-ViT-H-14-laion2B-s32B-b79K').to('cuda:2')\n",
    "openclip_tokenizer = AutoTokenizer.from_pretrained('laion/CLIP-ViT-H-14-laion2B-s32B-b79K')\n",
    "\n",
    "_, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 1.0167, -0.1938, -0.2867,  ..., -0.3315,  0.7034,  0.6887],\n",
       "         [-0.2502,  0.0163,  0.0645,  ...,  0.3769,  0.0875, -0.5767],\n",
       "         [ 0.0457,  0.1948,  0.3860,  ...,  0.5367, -0.0246, -0.0176],\n",
       "         ...,\n",
       "         [ 0.0633,  0.3087,  0.1527,  ...,  0.3461,  0.0585, -0.7915],\n",
       "         [ 0.2746,  0.2856,  0.1781,  ...,  0.2916, -0.1078, -0.4006],\n",
       "         [ 0.0927,  0.0650,  0.2563,  ...,  0.5321, -0.3691, -0.2808]]],\n",
       "       device='cuda:2', grad_fn=<AddBackward0>), pooler_output=tensor([[ 1.4526, -0.8311, -0.6829,  ..., -0.8668,  1.0815,  1.1504]],\n",
       "       device='cuda:2', grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# openclip_model.base_model(torch.rand((3, 224, 244)))\n",
    "openclip_model.vision_model(torch.rand((1, 3, 224, 224)).to('cuda:2'))\n",
    "# openclip_model.vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('../private-cv/test-images-small/')[0]\n",
    "# dir(openclip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(Image.open('../private-cv/test-images-small/f3fb94d15811b0.png')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenClipVisualDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img_tensor = preprocess(Image.open(path))\n",
    "        return img_tensor, path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "root = '../private-cv/test-images-small/'\n",
    "paths = [os.path.join(root, el) for el in os.listdir(root)]\n",
    "dataloader = torch.utils.data.DataLoader(dataset=OpenClipVisualDataset(paths), batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:33<00:00,  8.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "results = []\n",
    "for batch in tqdm(dataloader):\n",
    "    images_tensors, hashes = batch\n",
    "    with torch.no_grad():\n",
    "        out = openclip_model.vision_model(images_tensors.to('cuda:2'))['pooler_output']\n",
    "        out =  openclip_model.visual_projection(out)\n",
    "        out = projection(out)\n",
    "    out = out.detach().cpu().numpy().tolist()\n",
    "    for img_hash, emb in zip(hashes, out):\n",
    "        for i, val in enumerate(emb):\n",
    "            results.append((f'{img_hash}_{i}', val))\n",
    "    # raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results, columns=['id', 'emb']).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# laion/CLIP-ViT-H-14-laion2B-s32B-b79K\n",
    "# _, _, _preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')\n",
    "# _tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n",
    "\n",
    "# image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0)\n",
    "# text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "\n",
    "# with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "#     image_features = model.encode_image(image)\n",
    "#     text_features = model.encode_text(text)\n",
    "#     image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "#     text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#     text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
